
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>AIbro Documentation</title>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="source/javascripts/app/_copy.js"></script>
    <style media="screen">
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cpf {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s, .highlight .sa, .highlight .dl {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf, .highlight .fm {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv, .highlight .vm {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .nl {
  color: #f92672;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <style media="print">
      * {
        -webkit-transition:none!important;
        transition:none!important;
      }
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight, .highlight .w {
  color: #586e75;
}
.highlight .err {
  color: #002b36;
  background-color: #dc322f;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cm, .highlight .cpf, .highlight .c1, .highlight .cs {
  color: #657b83;
}
.highlight .cp {
  color: #b58900;
}
.highlight .nt {
  color: #b58900;
}
.highlight .o, .highlight .ow {
  color: #93a1a1;
}
.highlight .p, .highlight .pi {
  color: #93a1a1;
}
.highlight .gi {
  color: #859900;
}
.highlight .gd {
  color: #dc322f;
}
.highlight .gh {
  color: #268bd2;
  background-color: #002b36;
  font-weight: bold;
}
.highlight .k, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kv {
  color: #6c71c4;
}
.highlight .kc {
  color: #cb4b16;
}
.highlight .kt {
  color: #cb4b16;
}
.highlight .kd {
  color: #cb4b16;
}
.highlight .s, .highlight .sb, .highlight .sc, .highlight .dl, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .s1 {
  color: #859900;
}
.highlight .sa {
  color: #6c71c4;
}
.highlight .sr {
  color: #2aa198;
}
.highlight .si {
  color: #d33682;
}
.highlight .se {
  color: #d33682;
}
.highlight .nn {
  color: #b58900;
}
.highlight .nc {
  color: #b58900;
}
.highlight .no {
  color: #b58900;
}
.highlight .na {
  color: #268bd2;
}
.highlight .m, .highlight .mb, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mx {
  color: #859900;
}
.highlight .ss {
  color: #859900;
}
    </style>
    <link href="stylesheets/screen-1d8a2b99.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print-953e3353.css" rel="stylesheet" media="print" />
      <script src="javascripts/all-a61cc39a.js"></script>

    <script>
      $(function() { setupCodeCopy(); });
    </script>
  </head>

  <body class="index" data-languages="[&quot;python&quot;]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar-cad8cdcb.png" alt="" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo-5db5870f.png" class="logo" alt="" />
        <div class="lang-selector">
              <a href="#" data-language-name="python">python</a>
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <ul id="toc" class="toc-list-h1">
          <li>
            <a href="#introduction-aibro-inference" class="toc-h1 toc-link" data-title="Introduction - AIbro Inference">Introduction - AIbro Inference</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#why-aibro-inference" class="toc-h2 toc-link" data-title="Why AIbro Inference?">Why AIbro Inference?</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#authentication" class="toc-h1 toc-link" data-title="Authentication">Authentication</a>
          </li>
          <li>
            <a href="#start-the-first-inference-job-on-aibro" class="toc-h1 toc-link" data-title="Start The First Inference Job on AIbro">Start The First Inference Job on AIbro</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#step-1-install" class="toc-h2 toc-link" data-title="Step 1: Install">Step 1: Install</a>
                  </li>
                  <li>
                    <a href="#step-2-prepare-repository" class="toc-h2 toc-link" data-title="Step 2: Prepare repository">Step 2: Prepare repository</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#predict-py" class="toc-h3 toc-link" data-title="<strong>predict.py</strong>"><strong>predict.py</strong></a>
                              <ul class="toc-list-h4">
                                  <li>
                                    <a href="#load_model" class="toc-h4 toc-link" data-title="<em>load_model()</em>:"><em>load_model()</em>:</a>
                                  </li>
                                  <li>
                                    <a href="#run" class="toc-h4 toc-link" data-title="<em>run()</em>:"><em>run()</em>:</a>
                                  </li>
                              </ul>
                          </li>
                          <li>
                            <a href="#39-model-39-and-39-data-39-folders" class="toc-h3 toc-link" data-title="<strong>'model' and 'data' folders</strong>"><strong>'model' and 'data' folders</strong></a>
                          </li>
                          <li>
                            <a href="#requirement-txt" class="toc-h3 toc-link" data-title="<strong>requirement.txt</strong>"><strong>requirement.txt</strong></a>
                          </li>
                          <li>
                            <a href="#other-artifacts" class="toc-h3 toc-link" data-title="<strong>Other Artifacts</strong>"><strong>Other Artifacts</strong></a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#step-3-test-the-repo-using-dryrun" class="toc-h2 toc-link" data-title="Step 3: Test the Repo using Dryrun">Step 3: Test the Repo using Dryrun</a>
                  </li>
                  <li>
                    <a href="#step-4-create-an-inference-api-with-one-line-code" class="toc-h2 toc-link" data-title="Step 4: Create an inference API with one-line code">Step 4: Create an inference API with one-line code</a>
                  </li>
                  <li>
                    <a href="#step-5-test-an-aibro-api-with-curl" class="toc-h2 toc-link" data-title="Step 5: Test an Aibro API with curl:">Step 5: Test an Aibro API with curl:</a>
                  </li>
                  <li>
                    <a href="#step-6-limit-api-access-to-specific-clients-optional" class="toc-h2 toc-link" data-title="Step 6: Limit API Access to Specific Clients (Optional)">Step 6: Limit API Access to Specific Clients (Optional)</a>
                  </li>
                  <li>
                    <a href="#step-7-complete-job" class="toc-h2 toc-link" data-title="Step 7: Complete Job">Step 7: Complete Job</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#aibro-inference-methods" class="toc-h1 toc-link" data-title="aibro.inference Methods">aibro.inference Methods</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#deploy" class="toc-h2 toc-link" data-title="deploy()">deploy()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#complete" class="toc-h2 toc-link" data-title="complete()">complete()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-2" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#update_clients" class="toc-h2 toc-link" data-title="update_clients()">update_clients()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-3" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#list_clients" class="toc-h2 toc-link" data-title="list_clients()">list_clients()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-4" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#standby-and-cooling-instances" class="toc-h2 toc-link" data-title="Standby and Cooling instances">Standby and Cooling instances</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#definitions" class="toc-h3 toc-link" data-title="Definitions:">Definitions:</a>
                          </li>
                          <li>
                            <a href="#configure-for-non-uniform-traffic" class="toc-h3 toc-link" data-title="Configure for non-uniform traffic">Configure for non-uniform traffic</a>
                          </li>
                          <li>
                            <a href="#configure-for-high-traffic" class="toc-h3 toc-link" data-title="Configure for high traffic">Configure for high traffic</a>
                          </li>
                          <li>
                            <a href="#case-study-calculate-the-saving" class="toc-h3 toc-link" data-title="Case study: calculate the saving">Case study: calculate the saving</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#inference-job-amp-instance-status" class="toc-h2 toc-link" data-title="Inference Job &amp; Instance Status">Inference Job &amp; Instance Status</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#cloud-instance" class="toc-h1 toc-link" data-title="Cloud Instance">Cloud Instance</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#spot-vs-on-demand-instance" class="toc-h2 toc-link" data-title="Spot Vs On-demand Instance">Spot Vs On-demand Instance</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#machine-id" class="toc-h3 toc-link" data-title="Machine id">Machine id</a>
                          </li>
                          <li>
                            <a href="#pricing" class="toc-h3 toc-link" data-title="Pricing">Pricing</a>
                          </li>
                          <li>
                            <a href="#availability" class="toc-h3 toc-link" data-title="Availability">Availability</a>
                          </li>
                          <li>
                            <a href="#reliability" class="toc-h3 toc-link" data-title="Reliability">Reliability</a>
                          </li>
                      </ul>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#contact-us" class="toc-h1 toc-link" data-title="Contact Us">Contact Us</a>
          </li>
          <li>
            <a href="#data-privacy" class="toc-h1 toc-link" data-title="Data Privacy">Data Privacy</a>
          </li>
      </ul>
        <ul class="toc-footer">
            <li><a href='https://aipaca.ai'>Sign Up to Get Start</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <!-- ### HTTP Request

`GET http://example.com/api/kittens`

### Query Parameters

| Parameter    | Default | Description                                                                      |
| ------------ | ------- | -------------------------------------------------------------------------------- |
| include_cats | false   | If set to true, the result will also include cats.                               |
| available    | true    | If set to false, the result will include kittens that have already been adopted. | -->

<!-- <aside class="success">
Remember — a happy kitten is an authenticated kitten!
</aside>

## Get a Specific Kitten

```ruby
require 'kittn'

api = Kittn::APIClient.authorize!('meowmeowmeow')
api.kittens.get(2)
```

```python
import kittn

api = kittn.authorize('meowmeowmeow')
api.kittens.get(2)
```

```shell
curl "http://example.com/api/kittens/2" \
  -H "Authorization: meowmeowmeow"
```

```javascript
const kittn = require("kittn");

let api = kittn.authorize("meowmeowmeow");
let max = api.kittens.get(2);
```

> The above command returns JSON structured like this:

```json
{
  "id": 2,
  "name": "Max",
  "breed": "unknown",
  "fluffiness": 5,
  "cuteness": 10
}
```

This endpoint retrieves a specific kitten.

<aside class="warning">Inside HTML code blocks like this one, you can't use Markdown, so use <code>&lt;code&gt;</code> blocks to denote code.</aside>

### HTTP Request

`GET http://example.com/kittens/<ID>`

### URL Parameters

| Parameter | Description                      |
| --------- | -------------------------------- |
| ID        | The ID of the kitten to retrieve |

## Delete a Specific Kitten

```ruby
require 'kittn'

api = Kittn::APIClient.authorize!('meowmeowmeow')
api.kittens.delete(2)
```

```python
import kittn

api = kittn.authorize('meowmeowmeow')
api.kittens.delete(2)
```

```shell
curl "http://example.com/api/kittens/2" \
  -X DELETE \
  -H "Authorization: meowmeowmeow"
```

```javascript
const kittn = require("kittn");

let api = kittn.authorize("meowmeowmeow");
let max = api.kittens.delete(2);
```

> The above command returns JSON structured like this:

```json
{
  "id": 2,
  "deleted": ":("
}
```

This endpoint deletes a specific kitten.

### HTTP Request

`DELETE http://example.com/kittens/<ID>`

### URL Parameters

| Parameter | Description                    |
| --------- | ------------------------------ |
| ID        | The ID of the kitten to delete | -->
<h1 id='introduction-aibro-inference'>Introduction - AIbro Inference</h1>
<p><strong>AIbro Version</strong>: 1.1.1 <span style="color:blue;">(alpha)</span></p>

<p><strong>Last Documentation Update</strong>: Feb. 3, 2022</p>

<p><strong>Definition</strong>: API embedded python library</p>

<p>AIbro is a serverless MLOps tool that helps data scientists train &amp; deploy AI models on cloud platforms in 2 minutes.</p>

<p>The first step in the process is training a machine learning model, whereas this document focuses on the second step, cloud-based machine learning inference. If you are also interested in learning about training a model, please refer to the <a href="https://doc.aipaca.ai/training">training document</a>.</p>
<h2 id='why-aibro-inference'>Why AIbro Inference?</h2>
<ul>
<li>Cost-effectiveness. Compared to ordinary on-demand instances, Albro uses <a href="#spot-vs-on-demand-instance">spot</a> and <a href="#standby-and-cooling-instances">standby</a> instances to achieve 85% savings.</li>
<li>Ease of use. Aibro is easy to use and it only takes two minutes to deploy a machine learning model.</li>
<li>Flexibility. Our platform supports virtually all of the popular model frameworks in the market.</li>
</ul>
<h1 id='authentication'>Authentication</h1>
<p>Authentication is required when Aibro APIs are called for the first time.</p>

<p>Albro allows one of the following two ways to authenticate:</p>

<ul>
<li>Access token (<strong>recommend</strong>): This is supplied using a series of characters prefixed by &quot;secret_...&quot;, and can be found in the Aibro Console on the <a href="https://aipaca.ai/humming">humming page</a>.</li>
<li>Email &amp; password: These credentials are the same that were used to register on the <a href="https://aipaca.ai">AIbro website</a>.</li>
</ul>

<p><em>ps: &quot;humming&quot; is literally how an alpaca &quot;greets&quot; people</em>😉</p>
<h1 id='start-the-first-inference-job-on-aibro'>Start The First Inference Job on AIbro</h1>
<aside class="success">
Play around with the executable Colab tutorial <a href = "https://colab.research.google.com/drive/1NH2Aj1bbCgqJXyNKK9TkzwlO4JDBkTQC?usp=sharing"> here</a>
</aside>
<h2 id='step-1-install'>Step 1: Install</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">aibro</span>
</code></pre></div>
<p>The first step is to install the <a href="https://pypi.org/project/aibro/">Aibro Python library</a> using <code>pip</code>.</p>

<p>During the installation, if the <code>OSError: protocol now found</code> message appears, then it indicates an error caused by a missing file that can be easily resolved. The missing file is <code>/etc/protocols</code>, and entering the following command should remedy it.</p>

<p><code>sudo apt-get -o Dpkg::Options::=&quot;--force-confmiss&quot; install --reinstall netbase</code></p>
<h2 id='step-2-prepare-repository'>Step 2: Prepare repository</h2>
<p>The second step is to prepare a formatted inference model repository. The following instructions and source code can be found on our GitHub page, <a href="https://github.com/AIpaca-Inc/Aibro-examples">Aibro-examples</a>.</p>

<p>The repo should be structured using the following format:</p>

<p>repo <br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#predict-py">predict.py</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#39-model-39-and-39-data-39-folders">model</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#39-model-39-and-39-data-39-folders">data</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#requirement-txt">requirement.txt</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#other-artifacts">other artifacts</a><br/></p>
<h3 id='predict-py'><strong>predict.py</strong></h3>
<p>This is the entry point that will be called by Aibro. It should contain two methods, <code>load_model()</code> and <code>run(...)</code>.</p>
<h4 id='load_model'><em>load_model()</em>:</h4><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">load_model</span><span class="p">():</span>
    <span class="c1"># Portuguese to English translator
</span>    <span class="n">translator</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">saved_model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'model'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">translator</span>
</code></pre></div>
<p>This method is required to load and return your machine learning model from the <code>model</code> folder. As an example, a transformer-based Portuguese to English translator is used.</p>
<h4 id='run'><em>run()</em>:</h4>
<p>This method accepts the model as input. It loads the data from the <code>data</code> folder, generates predictions, and returns the results of the inference.</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">"./data/data.json"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"data"</span><span class="p">]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="s">"data"</span><span class="p">:</span> <span class="n">model</span><span class="p">(</span><span class="n">sentence</span><span class="p">).</span><span class="n">numpy</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div>
<p></br></br></br></br></br></br></br></p>

<p><em>test tip</em>: predict.py() should be able to return an inference result by:</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="n">run</span><span class="p">(</span><span class="n">load_model</span><span class="p">())</span>
</code></pre></div><h3 id='39-model-39-and-39-data-39-folders'><strong>&#39;model&#39; and &#39;data&#39; folders</strong></h3>
<p>There are no format restrictions on these two folders, as long as the input and output of <code>load_model()</code> and <code>run(...)</code> from <code>predict.py</code> are correct.</p>
<h3 id='requirement-txt'><strong>requirement.txt</strong></h3>
<p>Prior to starting model deployment, packages from <code>requirement.txt</code> are installed as part of setting up the environment.</p>

<p>NOTE: If your <code>requirement.txt</code> contains paths (such as <code>pandas @ file:///...</code>), then it is the subject of an open issue with <code>pip freeze</code> in version <code>20.1</code>. As a workaround, the following line can be used:</p>

<p><code>pip list --format=freeze &gt; requirements.txt</code></p>
<h3 id='other-artifacts'><strong>Other Artifacts</strong></h3>
<p>This refers to all of the other files and folders.</p>
<h2 id='step-3-test-the-repo-using-dryrun'>Step 3: Test the Repo using Dryrun</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro.inference</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">api_url</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
    <span class="n">artifacts_path</span> <span class="o">=</span> <span class="s">"./aibro_repo"</span><span class="p">,</span>
    <span class="n">dryrun</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>Dryrun locally validates the repo structure and tests to ensure that an inference result can be successfully returned.</p>
<h2 id='step-4-create-an-inference-api-with-one-line-code'>Step 4: Create an inference API with one-line code</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro.inference</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">api_url</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">"my_fancy_transformer"</span><span class="p">,</span>
    <span class="n">machine_id_config</span> <span class="o">=</span> <span class="s">"c5.large.od"</span><span class="p">,</span>
    <span class="n">artifacts_path</span> <span class="o">=</span> <span class="s">"./aibro_repo"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>Assume the formatted model repo is saved at path <code>&quot;/aibro_repo&quot;</code>. It can now be used to create an inference job. The model name must be unique with respect to all current <a href="https://aipaca.ai/inference_jobs">active inference jobs</a> under your profile.</p>

<p>In this example, we deploy a public custom model from <code>&quot;./aibro_repo&quot;</code> called &quot;my_fancy_transformer&quot; on machine &quot;c5.large.od&quot;, using an access token for authentication.</p>

<p>Once the deployment is complete, an API URL is returned with the syntax: </br></p>

<ul>
<li><strong>http://api.aipaca.ai/v1/{username}/{client_id}/{model_name}/predict</strong> </br></li>
</ul>

<p><strong>{client_id}</strong>: if your inference job is public, <strong>{client_id}</strong> is simply &quot;public&quot;. Otherwise, <strong>{client_id}</strong> will indicate one of your <a href="#add-clients">clients&#39; ID</a>.</p>

<p>In this tutorial, the API URL is:</p>

<ul>
<li><strong>http://api.aipaca.ai/v1/{username}/public/my_fancy_transformer/predict</strong> </br></li>
</ul>
<h2 id='step-5-test-an-aibro-api-with-curl'>Step 5: Test an Aibro API with curl:</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="s">"http://api.aipaca.ai/v1/{username}/public/my_fancy_transformer/predict"</span> <span class="o">-</span><span class="n">d</span> <span class="s">'{"data": "Olá"}'</span>
<span class="c1"># replace the {username} by your own
</span></code></pre></div>
<p>In this example, we demonstrate the use of the Aibro API using the <code>curl</code> utility. However, feel free to use whatever API tool you feel comfortable with.</p>

<p><strong>Note:</strong> The syntax when using <code>curl</code> depends on the file type in the <code>data</code> folder. In this tutorial, we use a <code>JSON</code> file.</p>

<table><thead>
<tr>
<th>File Type</th>
<th>syntax</th>
</tr>
</thead><tbody>
<tr>
<td>json</td>
<td>curl -X POST {{api url}} -d &#39;{&quot;your&quot;: &quot;data&quot;}&#39;<br/>curl -X POST {{api url}} -F file=@&#39;path/to/json/file&#39;</td>
</tr>
<tr>
<td>txt</td>
<td>curl -X POST {{api url}} -d &#39;your data&#39;<br/>curl -X POST {{api url}} -F file=@&#39;path/to/txt/file&#39;</td>
</tr>
<tr>
<td>csv</td>
<td>curl -X POST {{api url}} -F file=@&#39;path/to/csv/file&#39;</td>
</tr>
<tr>
<td>others</td>
<td>curl -X POST {{api url}} -F file=@&#39;path/to/zip/file&#39;</td>
</tr>
</tbody></table>

<p>You may have observed some patterns from the syntax lookup table above. The rules are summarized as follows:</p>

<ul>
<li>If the file type is <code>JSON</code> or <code>TXT</code>, you can use the <code>-d</code> flag to post the string data directly.</li>
<li>If the file type is either <code>JSON</code>, <code>TXT</code>, or <code>CSV</code>, you can use the <code>-F</code> flag to post the data file, specified by its path.</li>
<li>If the file type is something other than <code>JSON</code>, <code>TXT</code>, or <code>CSV</code>, then you have the option of using <code>ZIP</code> to archive the entire data folder and post it using its path.</li>
</ul>

<p><strong>Important!</strong> The posted data will replace everything in the <code>data</code> folder. Therefore, the data that you post should have the same format as what was originally there.</p>

<p><em>Tips</em>: If your inference time is more than one minute, we recommend either reducing the data size or increasing the <code>--keepalive-time</code> value when using <code>curl</code>.</p>
<h2 id='step-6-limit-api-access-to-specific-clients-optional'>Step 6: Limit API Access to Specific Clients (Optional)</h2>
<p>As the API owner, you have the option of restricting access to specific clients by assigning them a unique ID. This mitigates the risk of receiving an overwhelming number of API requests from different clients. The client ID is included with API endpoints, as shown in step 4. If no client ID is added, this inference job will be public.</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro.inference</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">Inference</span><span class="p">.</span><span class="n">update_clients</span><span class="p">(</span>
    <span class="n">job_id</span><span class="p">,</span>
    <span class="n">add_client_ids</span> <span class="o">=</span> <span class="p">[</span><span class="s">"client_1"</span><span class="p">,</span> <span class="s">"client_2"</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div><h2 id='step-7-complete-job'>Step 7: Complete Job</h2>
<p>Once the inference job is no longer required, to avoid unnecessary costs, please remember to close it with the <code>Inference.complete()</code> method.</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro.inference</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">Inference</span><span class="p">.</span><span class="n">complete</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
</code></pre></div><h1 id='aibro-inference-methods'>aibro.inference Methods</h1><h2 id='deploy'>deploy()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">deploy</span><span class="p">(</span>
    <span class="n">artifacts_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">machine_id_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">dryrun</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">cool_down_period_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">,</span>
    <span class="n">client_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">""</span><span class="p">,</span>
    <span class="n">wait_request_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</code></pre></div>
<p>The <code>deploy()</code> method starts an inference job, deploying models on cloud instances.</p>
<h3 id='parameters'>Parameters</h3>
<p><strong>artifacts_path</strong>: <em>Union[str, list]</em> <br/>
The path to the formatted repository. The input type can be either of type <strong>string</strong> or <strong>list</strong>:</p>

<table><thead>
<tr>
<th>Type</th>
<th>Syntax</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>str</td>
<td><code>&quot;path/to/repo&quot;</code></td>
<td>The whole repo would be uploaded and deployed on cloud</td>
</tr>
<tr>
<td>list</td>
<td><code>[&quot;./path/to/model&quot;,</code><br/> <code>&quot;./path/to/predict.py&quot;,</code><br/> <code>&quot;./path/to/data&quot;,</code><br/> <code>&quot;./path/to/requirements.txt&quot;,</code><br/> <code>&quot;./path/to/other_artifacts&quot;]</code></td>
<td>Select important artifacts one-by-one. Aibro will combine and recreate a model repository called <code>aibro_repo</code> under the root path of aibro library</td>
</tr>
</tbody></table>

<p><strong>model_name</strong>: <em>str</em> = None <br/>
This parameter specifies the model name used in the deployed inference job. It can only be set to None if, and only if dryrun is set to <code>False</code>. Within the scope of the user profile, the model name should be unique with respect to those among the active inference jobs.</p>

<p><strong>machine_id_config</strong>: <em>Union[str, dict]</em> <br/>
This parameter specifies the machine configuration to be used to deploy the model. In the configuration, machines are categorized as either <strong>Standby</strong> or <strong>Cooling</strong>.</p>

<ul>
<li>A <strong>Standby</strong> instance is one that never turns off.</li>
<li>A <strong>Cooling</strong> instance is one that only turns on when it receives a new inference request, and only turns off when no inference request is received within <code>cool_down_period_s</code> seconds.</li>
</ul>

<p>The macine_id_config can be set to <code>None</code> if and only <code>dryrun</code> is set to <code>False</code>. The input type can be either a string or a dictionary:</p>

<table><thead>
<tr>
<th>Type</th>
<th>Syntax</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>str</td>
<td><code>&quot;c5.large.od&quot;</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance</td>
</tr>
<tr>
<td>dict</td>
<td><code>{&quot;standby&quot;: &quot;c5.large.od&quot;}</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance</td>
</tr>
<tr>
<td>dict</td>
<td><code>{&quot;standby&quot;: &quot;c5.large.od&quot;, &quot;cooling&quot;: &quot;g4dn.4xlarge.od&quot;}</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance and the on-demand &quot;g4dn.4xlarge&quot; instance as the cooling instance</td>
</tr>
</tbody></table>

<p><u>Important:</u></p>

<ol>
<li><u>The machine id has to be on-demand (ends by <code>.od</code>).</u></li>
<li><u>One standby instance is mandatory whereas cooling instance is optional (e.g. syntax such as {&quot;cooling&quot;: &quot;g4dn.4xlarge.od&quot;} is invalid).</u></li>
</ol>

<p><strong>For more details about the usage of standby and cooling instances, check out the <a href="#standby-and-cooling-instances">section below</a>.</strong></p>

<aside class="notice">
<span style="font-weight: bold">FAQ: Why does `machine_id_config` only accept on-demand instances? Isn't it more expensive than using a spot instance?</span> <br/>

Answer: Spot instances are not always available, so on-demand instances are required in order to guarantee that the job can always get a server. If spot instances are available, Albro automatically replaces its same-type on-demand instances to operate more cost-effectively.

</aside>

<p><strong>dryrun</strong>: <em>bool = False</em><br/>
The <code>dryrun</code> option is used to perform a local test of the model. When set to <code>True</code>, the <code>deploy()</code> method will validate the structure of the repository and test whether the inference result can be successfully returned.</p>

<p><strong>cool_down_period_s</strong>: <em>int = 600</em><br/>
This parameter specifies the cool-down period of a cooling instance. By default, the cooling instance will stop after there have been no new inference requests for 600 seconds (10 minutes).</p>

<p><strong>client_ids</strong>: <em>List[str] = []</em><br/>
This argument is used to restrict access to your inference API, for use only by specific clients. The client IDs can be customized in any syntax, provided that there is no duplication. If there are no client IDs specified then the inference job is public.</p>

<p>The <code>Inference.update_clients()</code> method can be used to add/remove client IDs. As mentioned in the tutorial, the client ID is used as a part of the API URL.</p>

<p>The table below defines each role.</p>

<table><thead>
<tr>
<th>Role</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>API owner</td>
<td>The person that created the inference</td>
</tr>
<tr>
<td>API client</td>
<td>Individuals that have access to the inference</td>
</tr>
</tbody></table>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token is used to <a href="#authentication">authenticate</a> the API request. If its value is <code>None</code>, the client’s email and password are required for the request to be accepted.</p>

<p><strong>description</strong>: <em>str</em> = &quot;&quot; <br/>
The description is used to briefly explain the inference, allowing users to better distinguish them.</p>

<p><strong>wait_request_s</strong>: <em>int = 30</em><br/>
This parameter specifies the number of seconds that Aibro will wait for instance requests to be fulfilled.</p>
<h2 id='complete'>complete()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">complete</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">):</span>
</code></pre></div>
<p>The <code>complete()</code> method signals the end of an inference, shutting down all of its API services.</p>
<h3 id='parameters-2'>Parameters</h3>
<p><strong>model_name</strong>: <em>str</em> = None <br/>
This parameter is used to identify the inference job to be stopped. Since a model name is required to be unique, it can be used instead of the <code>job_id</code> to locate it. If values for both <code>model_name</code> and <code>job_id</code> are specified then the <code>job_id</code> will be used for the search.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
This identifies the deployed inference job using the job ID. Specifying an ID will cause the <code>model_name</code> parameter to be ignored.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token is used to <a href="#authentication">authenticate</a> the API request. If its value is None, the client’s email and password are required for the request to be accepted.</p>
<h2 id='update_clients'>update_clients()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">update_clients</span><span class="p">(</span>
        <span class="n">add_client_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">remove_client_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">be_public</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span><span class="o">-&gt;</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</code></pre></div>
<p>The <code>update_clients()</code> method is used to modify the list of authorized client IDs for the inference job.</p>
<h3 id='parameters-3'>Parameters</h3>
<p><strong>add_client_ids</strong>: <em>Union[str, List[str]]</em> = [] <br/>
This parameter will add a single client ID, or a list of client IDs. If the duplicate IDs are found within the list or have already been authorized then the update will be canceled.</p>

<p><strong>remove_client_ids</strong>: <em>Union[str, List[str]]</em> = [] <br/>
This parameter will remove a single client ID or a list of client IDs from the set of authorized clients. IDs that are not found will be ignored.</p>

<p><strong>be_public</strong>: <em>bool</em> = False <br/>
Setting the <code>be_public</code> parameter to <code>True</code> removes all of the client IDs from the access list, leaving the inference job accessible to all.</p>

<p><strong>model_name</strong>: <em>str</em> = None <br/>
This parameter is used to identify the model within the inference job. Since a model name is required to be unique, it can be used instead of the <code>job_id</code> to locate it. If values for both <code>model_name</code> and <code>job_id</code> are specified then the <code>job_id</code> will be used for the search.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
This identifies the deployed inference job using the job ID. Specifying an ID will cause the <code>model_name</code> parameter to be ignored.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token is used to <a href="#authentication">authenticate</a> the API request. If its value is <code>None</code>, the client’s email and password are required for the request to be accepted.</p>
<h2 id='list_clients'>list_clients()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">list_clients</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span><span class="o">-&gt;</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</code></pre></div>
<p>The <code>list_clients()</code> method returns a list of client IDs for a specific inference job.</p>
<h3 id='parameters-4'>Parameters</h3>
<p><strong>model_name</strong>: <em>str</em> = None <br/>
This parameter is used to identify the model within the inference job. Since a model name is required to be unique, it can be used instead of the <code>job_id</code> to locate it. If values for both <code>model_name</code> and <code>job_id</code> are specified then the job_id will be used for the search.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
This identifies the deployed inference job using the job ID. Specifying an ID will cause the <code>model_name</code> parameter to be ignored.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token is used to <a href="#authentication">authenticate</a> the API request. If its value is <code>None</code>, the client’s email and password are required for the request to be accepted.</p>
<h2 id='standby-and-cooling-instances'>Standby and Cooling instances</h2><h3 id='definitions'>Definitions:</h3>
<p>Instances are classified in one of two ways; <strong>Standby</strong> or <strong>Cooling</strong>.</p>

<ul>
<li>A <strong>Standby</strong> instance is one that never turns off.</li>
<li>A <strong>Cooling</strong> instance is one that only turns on when it receives a new inference request, and only turns off when no inference request is received within <code>cool_down_period_s</code> seconds.</li>
</ul>

<p>If both standby and cooling instances are turned on, the cooling instance would have a higher priority to handle upcoming inference requests.</p>
<h3 id='configure-for-non-uniform-traffic'>Configure for non-uniform traffic</h3>
<p>Dealing with non-uniform traffic and having the ability to scale are important features of the system, and it is helpful to understand how sporadic and non-uniform traffic can affect your strategy. In simple words, the combination of standby and cooling instances, properly configured, will lead you toward optimal pricing for your usage.</p>

<p>Consider that you have an inference API for a web application and the call frequency is proportional to the site traffic. If the traffic is typically non-uniform, such as the case where there is more traffic during the day and relatively little during the night, then the configuration should be set accordingly. To take advantage of known traffic patterns, in this case, you might set a CPU or cheaper GPU instance as the standby instance, and dedicate a powerful GPU for the cooling instance. In this approach, the powerful cooling instance would efficiently handle intensive traffic during the day and the standby instance would save you on costs because it is running at night. Importantly, this balance maintains the near real-time performance.</p>
<h3 id='configure-for-high-traffic'>Configure for high traffic</h3>
<p>On the other hand, some sites have traffic that is uniform. Constant traffic is common, for example, in applications that are depended on by clients operating in many different time zones. If your site traffic is more uniform, and people never stop using it, using standby instances with a powerful GPU is the recommended configuration.</p>

<p>The reason for this becomes clear when you consider the previous configuration, which uses a weak standby processor and a powerful cooling one. With constant traffic, the cooling instance would never shut down because the timeout period would never expire. Consequently, the lesser-powered standby processor would not be utilized and thus wasted.</p>
<h3 id='case-study-calculate-the-saving'>Case study: calculate the saving</h3>
<p><img src="https://drive.google.com/uc?export=view&amp;id=1sLxQ6SZDamjT9qGp2JE7zElUZwIpLQ3I" alt="" /></p>

<p><strong>Graph explanation</strong>: This graph describes an inference job with configuration <code>machine_id_config = machine_id_config = {&quot;standby&quot;: &quot;g4dn.4xlarge&quot;, &quot;cooling&quot;: &quot;g4dn.12xlarge.od&quot;}</code>. The job was operated between 8:00 am and 11.59 pm, and it is clear that most of the inference requests (IRs) were received during the morning and afternoon.</p>

<p>Every time an IR was received by the cooling instance, the cooldown period reset. If the cooldown period passed by without receiving an IR, the instance stopped, and the standby instance handled the next IR. At that time, the standby invoked the previously stopped cooling instance. During the period it took the cooling instance to fully come online, the standby instance processed the IRs.</p>

<p>Over the course of the 16 hours, cooling instances were turned on for 6 hours and the standby instance was never turned off. The relevant machine pricing is shown below:</p>

<table><thead>
<tr>
<th>Machine id</th>
<th>Pricing</th>
</tr>
</thead><tbody>
<tr>
<td>g4dn.12xlarge.od</td>
<td>$3.91/hr</td>
</tr>
<tr>
<td>g4dn.4xlarge.od</td>
<td>$1.20/hr</td>
</tr>
<tr>
<td>g4dn.12xlarge</td>
<td>$1.61/hr</td>
</tr>
<tr>
<td>g4dn.4xlarge</td>
<td>$0.36/hr</td>
</tr>
</tbody></table>

<p><strong>Let&#39;s compare the savings with and without the hybrid configuration.</strong></p>

<table><thead>
<tr>
<th>Configuration</th>
<th>machine_id_config</th>
<th>Cost</th>
<th>Spot Cost</th>
</tr>
</thead><tbody>
<tr>
<td>Without standby&amp;cooling</td>
<td><code>{&quot;standby&quot;: &quot;g4dn.12xlarge.od&quot;}</code></td>
<td>3.91 * 16 = $62.56</td>
<td>$25.76</td>
</tr>
<tr>
<td>With standby&amp;cooling</td>
<td><code>{&quot;standby&quot;: &quot;g4dn.4xlarge&quot;,</code><br/><code>&quot;cooling&quot;: &quot;g4dn.12xlarge.od&quot;}</code></td>
<td>1.2_ 16 + 3.91 * 6 = $42.66</td>
<td>$15.42</td>
</tr>
</tbody></table>

<p>In this scenario, the savings from using the hybrid standby and cooling configuration was more than 30%. Furthermore, if spot instances were available over the period, Albro would further cut the cost from $62.56 to $15.42. This is a savings of more than 75%!</p>
<h2 id='inference-job-amp-instance-status'>Inference Job &amp; Instance Status</h2>
<p>Once a job starts, its states and substates are updated on the <a href="https://aipaca.ai/inference_jobs">Jobs page within the Aibro Console</a>.</p>

<table><thead>
<tr>
<th>Job Status</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>QUEUING</td>
<td>Waiting to be deployed</td>
</tr>
<tr>
<td>DEPLOYED</td>
<td>Inference API is ready to be used</td>
</tr>
<tr>
<td>CANCELED</td>
<td>Canceled due to some errors</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>Job was completed</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Job Substatus</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>REQUESTING SERVER</td>
<td>Requesting an instance to deploy models</td>
</tr>
<tr>
<td>CONNECTING SERVER</td>
<td>Connecting an initializing instance</td>
</tr>
<tr>
<td>GEARING UP ENV</td>
<td>Gearing up tensorflow and mounting GPUs</td>
</tr>
<tr>
<td>DEPLOYING MODEL</td>
<td>Deploying model</td>
</tr>
<tr>
<td>DEPLOYED MODEL</td>
<td>Model was deployed</td>
</tr>
<tr>
<td>CANCELED</td>
<td>Canceled due to errors</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>Completed the job</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Instance Status</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>LAUNCHING</td>
<td>Setting up instance for inference</td>
</tr>
<tr>
<td>EXECUTING</td>
<td>Jobs being processed and ready to receive requests</td>
</tr>
<tr>
<td>COOLING</td>
<td>Within cooling Period (must be a <a href="#standby-and-cooling-instances">cooling instance</a>)</td>
</tr>
<tr>
<td>CLOSING</td>
<td>Stopping/terminating instance</td>
</tr>
<tr>
<td>CLOSED</td>
<td>Instance has been stopped/terminated</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Instance Substatus</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>STOPPING/STOPPED</td>
<td>Shut down instance but retain root volume <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">Reference</a></td>
</tr>
<tr>
<td>TERMINATING/TERMINATED</td>
<td>Completely delete the instance <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html">Reference</a></td>
</tr>
</tbody></table>

<p>The following table is a status-substatus map of jobs and instances.</p>

<table><thead>
<tr>
<th>Job Status</th>
<th>Job Substatus</th>
<th>Instance Status</th>
<th>Instance Substatus</th>
</tr>
</thead><tbody>
<tr>
<td>QUEUING</td>
<td>REQUESTING SERVER</td>
<td></td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>CONNECTING SERVER</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>GEARING UP ENV</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>DEPLOYING MODEL</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>----------</td>
<td>------------</td>
<td>----------------------</td>
<td>--------------------------</td>
</tr>
<tr>
<td>DEPLOYED</td>
<td>DEPLOYED</td>
<td>EXECUTING/COOLING</td>
<td></td>
</tr>
<tr>
<td>----------</td>
<td>------------</td>
<td>----------------------</td>
<td>--------------------------</td>
</tr>
<tr>
<td>CANCELED</td>
<td>CANCELED</td>
<td>COOLING/CLOSING/CLOSED</td>
<td>COOLING/(STOPPING, TERMINATING)/(STOPPED, TERMINATED)</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>COMPLETED</td>
<td>COOLING/CLOSING/CLOSED</td>
<td>COOLING/(STOPPING, TERMINATING)/(STOPPED, TERMINATED)</td>
</tr>
</tbody></table>
<h1 id='cloud-instance'>Cloud Instance</h1>
<p>The words &quot;machine&quot;, &quot;server&quot;, and “instance” are used interchangeably in the following content.</p>
<h2 id='spot-vs-on-demand-instance'>Spot Vs On-demand Instance</h2><h3 id='machine-id'>Machine id</h3>
<p>By simply adding <code>&quot;.od&quot;</code> after the machine ID, it is converted from a spot instance to an on-demand instance (e.g. <em>p2.xlarge</em> is spot and <em>p2.xlarge.od</em> is on-demand).</p>
<h3 id='pricing'>Pricing</h3>
<p>Spot instances are usually 70% cheaper than their corresponding on-demand instances.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1G6fVuxUu8Yofj_SBxl57m_lFZ83L4MzE" alt="" />
<img src="https://drive.google.com/uc?export=view&amp;id=1vomqhv0C7-ZjmwZ62dv3bjFJZtL2-wiT" alt="" /></p>
<h3 id='availability'>Availability</h3>
<p>As a tradeoff, spot instance requests are not always fulfilled. We define <strong>&quot;availability&quot;</strong> as the success probability of an instance request.</p>

<p>Clearly, the availability of on-demand instances is always 100%. Provided there is sufficient capacity in the Aibro marketplace, an on-demand instance is guaranteed. There is a small chance that AWS will reach capacity and create a bottleneck, although this cannot be detected until the request encounters an error during a job.</p>

<p>The availability of spot instances is varied by instance type and request time. We have found that in general, the more powerful instance types have less availability (e.g. p3.2xlarge is less available than p2.xlarge). Not surprisingly, we have also found that spot instances are more often available during non-business hours.</p>
<h3 id='reliability'>Reliability</h3>
<p>When choosing a configuration, it is relevant that spot instances can be interrupted by AWS, whereas on-demand instances are always stable.</p>
<h1 id='contact-us'>Contact Us</h1>
<p>If you have comments, questions, or concerns then please reach out to us in one of the following ways:</p>

<ol>
<li><strong>Recommended</strong>: We are available through the <a href="https://discord.gg/kEdtjUYb">Discord Community</a> and you can direct message our support team.</li>
<li>Use the &quot;Contact Us&quot; button on our <a href="https://aipaca.ai">website</a></li>
<li>Send us a message using the <a href="#send_message"><code>aibro.comm.Comm.send_message()</code></a></li>
<li>Email us at <a href = "mailto: hello@aipaca.ai">hello@aipaca.ai</a></li>
</ol>
<h1 id='data-privacy'>Data Privacy</h1>
<p>Each Albro inference stores only the metadata from inference requests. This is done for the purpose of service improvement.</p>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
                <a href="#" data-language-name="python">python</a>
          </div>
      </div>
    </div>
  </body>
</html>
