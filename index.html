
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>AIbro Documentation</title>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="source/javascripts/app/_copy.js"></script>
    <style media="screen">
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cpf {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s, .highlight .sa, .highlight .dl {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf, .highlight .fm {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv, .highlight .vm {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .nl {
  color: #f92672;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <style media="print">
      * {
        -webkit-transition:none!important;
        transition:none!important;
      }
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight, .highlight .w {
  color: #586e75;
}
.highlight .err {
  color: #002b36;
  background-color: #dc322f;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cm, .highlight .cpf, .highlight .c1, .highlight .cs {
  color: #657b83;
}
.highlight .cp {
  color: #b58900;
}
.highlight .nt {
  color: #b58900;
}
.highlight .o, .highlight .ow {
  color: #93a1a1;
}
.highlight .p, .highlight .pi {
  color: #93a1a1;
}
.highlight .gi {
  color: #859900;
}
.highlight .gd {
  color: #dc322f;
}
.highlight .gh {
  color: #268bd2;
  background-color: #002b36;
  font-weight: bold;
}
.highlight .k, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kv {
  color: #6c71c4;
}
.highlight .kc {
  color: #cb4b16;
}
.highlight .kt {
  color: #cb4b16;
}
.highlight .kd {
  color: #cb4b16;
}
.highlight .s, .highlight .sb, .highlight .sc, .highlight .dl, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .s1 {
  color: #859900;
}
.highlight .sa {
  color: #6c71c4;
}
.highlight .sr {
  color: #2aa198;
}
.highlight .si {
  color: #d33682;
}
.highlight .se {
  color: #d33682;
}
.highlight .nn {
  color: #b58900;
}
.highlight .nc {
  color: #b58900;
}
.highlight .no {
  color: #b58900;
}
.highlight .na {
  color: #268bd2;
}
.highlight .m, .highlight .mb, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mx {
  color: #859900;
}
.highlight .ss {
  color: #859900;
}
    </style>
    <link href="stylesheets/screen-1d8a2b99.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print-953e3353.css" rel="stylesheet" media="print" />
      <script src="javascripts/all-c9c9fd69.js"></script>

    <script>
      $(function() { setupCodeCopy(); });
    </script>
  </head>

  <body class="index" data-languages="[&quot;python&quot;]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar-cad8cdcb.png" alt="" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo-5db5870f.png" class="logo" alt="" />
        <div class="lang-selector">
              <a href="#" data-language-name="python">python</a>
        </div>
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <ul id="toc" class="toc-list-h1">
          <li>
            <a href="#introduction-aibro-inference" class="toc-h1 toc-link" data-title="Introduction - AIbro Inference">Introduction - AIbro Inference</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#why-aibro-inference" class="toc-h2 toc-link" data-title="Why AIbro Inference?">Why AIbro Inference?</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#authentication" class="toc-h1 toc-link" data-title="Authentication">Authentication</a>
          </li>
          <li>
            <a href="#start-the-first-inference-job-on-aibro" class="toc-h1 toc-link" data-title="Start The First Inference Job on AIbro">Start The First Inference Job on AIbro</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#step-1-install" class="toc-h2 toc-link" data-title="Step 1: Install">Step 1: Install</a>
                  </li>
                  <li>
                    <a href="#step-2-prepare-a-formatted-model-repo" class="toc-h2 toc-link" data-title="Step 2: Prepare a formatted model repo">Step 2: Prepare a formatted model repo</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#predict-py" class="toc-h3 toc-link" data-title="<strong>predict.py</strong>"><strong>predict.py</strong></a>
                              <ul class="toc-list-h4">
                                  <li>
                                    <a href="#load_model" class="toc-h4 toc-link" data-title="<em>load_model()</em>:"><em>load_model()</em>:</a>
                                  </li>
                                  <li>
                                    <a href="#run" class="toc-h4 toc-link" data-title="<em>run()</em>:"><em>run()</em>:</a>
                                  </li>
                              </ul>
                          </li>
                          <li>
                            <a href="#39-model-39-and-39-data-39-folders" class="toc-h3 toc-link" data-title="<strong>'model' and 'data' folders</strong>"><strong>'model' and 'data' folders</strong></a>
                          </li>
                          <li>
                            <a href="#requirement-txt" class="toc-h3 toc-link" data-title="<strong>requirement.txt</strong>"><strong>requirement.txt</strong></a>
                          </li>
                          <li>
                            <a href="#other-artifacts" class="toc-h3 toc-link" data-title="<strong>Other Artifacts</strong>"><strong>Other Artifacts</strong></a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#step-3-test-the-repo-by-dryrun" class="toc-h2 toc-link" data-title="Step 3: Test the Repo by Dryrun">Step 3: Test the Repo by Dryrun</a>
                  </li>
                  <li>
                    <a href="#step-4-create-an-inference-api-with-one-line-code" class="toc-h2 toc-link" data-title="Step 4: Create an inference API with one-line code">Step 4: Create an inference API with one-line code</a>
                  </li>
                  <li>
                    <a href="#step-5-test-a-aibro-api-with-curl" class="toc-h2 toc-link" data-title="Step 5: Test a Aibro API with curl:">Step 5: Test a Aibro API with curl:</a>
                  </li>
                  <li>
                    <a href="#step-6-limit-api-access-to-specific-clients-optional" class="toc-h2 toc-link" data-title="Step 6: Limit API Access to Specific Clients (Optional)">Step 6: Limit API Access to Specific Clients (Optional)</a>
                  </li>
                  <li>
                    <a href="#step-7-complete-job" class="toc-h2 toc-link" data-title="Step 7: Complete Job">Step 7: Complete Job</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#aibro-inference" class="toc-h1 toc-link" data-title="aibro.Inference">aibro.Inference</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#deploy" class="toc-h2 toc-link" data-title="deploy()">deploy()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#complete" class="toc-h2 toc-link" data-title="complete()">complete()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-2" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#update_clients" class="toc-h2 toc-link" data-title="update_clients()">update_clients()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-3" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#list_clients" class="toc-h2 toc-link" data-title="list_clients()">list_clients()</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#parameters-4" class="toc-h3 toc-link" data-title="Parameters">Parameters</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#standby-and-cooling-instances" class="toc-h2 toc-link" data-title="Standby and Cooling instances">Standby and Cooling instances</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#definitions" class="toc-h3 toc-link" data-title="Definitions:">Definitions:</a>
                          </li>
                          <li>
                            <a href="#configure-for-non-uniform-traffic" class="toc-h3 toc-link" data-title="Configure for non-uniform traffic">Configure for non-uniform traffic</a>
                          </li>
                          <li>
                            <a href="#configure-for-high-traffic" class="toc-h3 toc-link" data-title="Configure for high traffic">Configure for high traffic</a>
                          </li>
                          <li>
                            <a href="#case-study-calculate-the-saving" class="toc-h3 toc-link" data-title="Case study: calculate the saving">Case study: calculate the saving</a>
                          </li>
                      </ul>
                  </li>
                  <li>
                    <a href="#inference-job-amp-instance-status" class="toc-h2 toc-link" data-title="Inference Job &amp; Instance Status">Inference Job &amp; Instance Status</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#cloud-instance" class="toc-h1 toc-link" data-title="Cloud Instance">Cloud Instance</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#spot-vs-on-demand-instance" class="toc-h2 toc-link" data-title="Spot Vs On-demand Instance">Spot Vs On-demand Instance</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#machine-id" class="toc-h3 toc-link" data-title="Machine id">Machine id</a>
                          </li>
                          <li>
                            <a href="#pricing" class="toc-h3 toc-link" data-title="Pricing">Pricing</a>
                          </li>
                          <li>
                            <a href="#availability" class="toc-h3 toc-link" data-title="Availability">Availability</a>
                          </li>
                          <li>
                            <a href="#reliability" class="toc-h3 toc-link" data-title="Reliability">Reliability</a>
                          </li>
                      </ul>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#contact-us" class="toc-h1 toc-link" data-title="Contact Us">Contact Us</a>
          </li>
          <li>
            <a href="#data-privacy" class="toc-h1 toc-link" data-title="Data Privacy">Data Privacy</a>
          </li>
      </ul>
        <ul class="toc-footer">
            <li><a href='https://aipaca.ai'>Sign Up to Get Start</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <!-- ### HTTP Request

`GET http://example.com/api/kittens`

### Query Parameters

| Parameter    | Default | Description                                                                      |
| ------------ | ------- | -------------------------------------------------------------------------------- |
| include_cats | false   | If set to true, the result will also include cats.                               |
| available    | true    | If set to false, the result will include kittens that have already been adopted. | -->

<!-- <aside class="success">
Remember â€” a happy kitten is an authenticated kitten!
</aside>

## Get a Specific Kitten

```ruby
require 'kittn'

api = Kittn::APIClient.authorize!('meowmeowmeow')
api.kittens.get(2)
```

```python
import kittn

api = kittn.authorize('meowmeowmeow')
api.kittens.get(2)
```

```shell
curl "http://example.com/api/kittens/2" \
  -H "Authorization: meowmeowmeow"
```

```javascript
const kittn = require("kittn");

let api = kittn.authorize("meowmeowmeow");
let max = api.kittens.get(2);
```

> The above command returns JSON structured like this:

```json
{
  "id": 2,
  "name": "Max",
  "breed": "unknown",
  "fluffiness": 5,
  "cuteness": 10
}
```

This endpoint retrieves a specific kitten.

<aside class="warning">Inside HTML code blocks like this one, you can't use Markdown, so use <code>&lt;code&gt;</code> blocks to denote code.</aside>

### HTTP Request

`GET http://example.com/kittens/<ID>`

### URL Parameters

| Parameter | Description                      |
| --------- | -------------------------------- |
| ID        | The ID of the kitten to retrieve |

## Delete a Specific Kitten

```ruby
require 'kittn'

api = Kittn::APIClient.authorize!('meowmeowmeow')
api.kittens.delete(2)
```

```python
import kittn

api = kittn.authorize('meowmeowmeow')
api.kittens.delete(2)
```

```shell
curl "http://example.com/api/kittens/2" \
  -X DELETE \
  -H "Authorization: meowmeowmeow"
```

```javascript
const kittn = require("kittn");

let api = kittn.authorize("meowmeowmeow");
let max = api.kittens.delete(2);
```

> The above command returns JSON structured like this:

```json
{
  "id": 2,
  "deleted": ":("
}
```

This endpoint deletes a specific kitten.

### HTTP Request

`DELETE http://example.com/kittens/<ID>`

### URL Parameters

| Parameter | Description                    |
| --------- | ------------------------------ |
| ID        | The ID of the kitten to delete | -->
<h1 id='introduction-aibro-inference'>Introduction - AIbro Inference</h1>
<p><strong>AIbro Version</strong>: 1.1.0 <span style="color:blue;">(alpha)</span></p>

<p><strong>Last Documentation Update</strong>: Dec. 12, 2021</p>

<p><strong>Definition</strong>: API embedded python library</p>

<p>AIbro is a serverless MLOps tool that helps data scientists train &amp; inference AI models on cloud platforms in 2 minutes.</p>

<p>This document focuses on cloud <strong>inference</strong>. If you are also interested in training, here is the <a href="https://doc.aipaca.ai/training">training document link</a>.</p>
<h2 id='why-aibro-inference'>Why AIbro Inference?</h2>
<ul>
<li>Reason 1 - cheap: compared to ordinary on-demand instances, AIbro uses <a href="#spot-vs-on-demand-instance">spot</a> and <a href="#get-more-saving-by-cooling-instance">standby</a> instances to achieve an average of 70%+ savings.</li>
<li>Reason 2 - easy: it only takes you 2 minutes to deploy a machine learning model.</li>
<li>Reason 3 - flexible: aibro supports basically all popular model frameworks in the market.</li>
</ul>
<h1 id='authentication'>Authentication</h1>
<p>Authentication is required when APIs are called for the first time.</p>

<p>AIbro allows one of the following two ways to verify authentication:</p>

<ul>
<li>Access token (<strong>recommend</strong>): a series of characters starts with &quot;secret_...&quot;; it can be found on the <a href="https://aipaca.ai/humming">humming page</a>.</li>
<li>Email &amp; password: same as you register at the AIbro <a href="https://aipaca.ai">website</a>.</li>
</ul>

<p><em>ps: &quot;humming&quot; is literally how an alpaca &quot;greets&quot; people</em>ðŸ˜‰</p>
<h1 id='start-the-first-inference-job-on-aibro'>Start The First Inference Job on AIbro</h1>
<aside class="success">
Play around with the executable Colab tutorial <a href = "https://colab.research.google.com/drive/1NH2Aj1bbCgqJXyNKK9TkzwlO4JDBkTQC?usp=sharing"> here</a>
</aside>
<h2 id='step-1-install'>Step 1: Install</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">aibro</span>
</code></pre></div>
<p>Install <a href="https://pypi.org/project/aibro/">aibro python library</a> by pip.</p>

<p>If <code>OSError: protocol not found</code> shows up, it is caused by missing <code>/etc/protocols</code> file. This command should be able to resolve the error: <code>sudo apt-get -o Dpkg::Options::=&quot;--force-confmiss&quot; install --reinstall netbase</code></p>
<h2 id='step-2-prepare-a-formatted-model-repo'>Step 2: Prepare a formatted model repo</h2>
<p>Source: <a href="https://github.com/AIpaca-Inc/AIbro_model_repo">https://github.com/AIpaca-Inc/AIbro_model_repo</a>.</p>

<p>The repo should be structured in the following format:</p>

<p>repo <br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#predict-py">predict.py</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#39-model-39-and-39-data-39-folders">model</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#39-model-39-and-39-data-39-folders">data</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#requirement-txt">requirement.txt</a><br/>
&nbsp;&nbsp;&nbsp;&nbsp;|__&nbsp;<a href="#other-artifacts">other artifacts</a><br/></p>
<h3 id='predict-py'><strong>predict.py</strong></h3>
<p>This is the entry point of AIbro.</p>

<p>predict.py should contain two methods:</p>
<h4 id='load_model'><em>load_model()</em>:</h4><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">load_model</span><span class="p">():</span>
    <span class="c1"># Portuguese to English translator
</span>    <span class="n">translator</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">saved_model</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'model'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">translator</span>
</code></pre></div>
<p>This method should load and return your machine learning model from the &quot;model&quot; folder. A transformer-based Portuguese to English translator is used in this example repo.</p>
<h4 id='run'><em>run()</em>:</h4><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">"./data/data.json"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"data"</span><span class="p">]</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span><span class="s">"data"</span><span class="p">:</span> <span class="n">model</span><span class="p">(</span><span class="n">sentence</span><span class="p">).</span><span class="n">numpy</span><span class="p">().</span><span class="n">decode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div>
<p>This method uses a model as the input, load data from the &quot;data&quot; folder, predict, then return the inference result.
</br></br></br></br></br></br></br></p>

<p><em>Local test tip</em>: predict.py() should be able to return an inference result by:</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="n">run</span><span class="p">(</span><span class="n">load_model</span><span class="p">())</span>
</code></pre></div><h3 id='39-model-39-and-39-data-39-folders'><strong>&#39;model&#39; and &#39;data&#39; folders</strong></h3>
<p>There is no format restriction on the &quot;model&quot; and &quot;data&quot; folder as long as the input and output of load_model() and run() from predict.py are correct.</p>
<h3 id='requirement-txt'><strong>requirement.txt</strong></h3>
<p>Before starting deploying the model, packages from requirement.txt are installed to set up the environment.</p>

<p>If your requirement.txt contains paths (such as <code>pandas @ file:///....</code>), it was an open issue with <code>pip freeze</code> in version <code>20.1</code>. You could use <code>pip list --format=freeze &gt; requirements.txt</code> as a workaround.</p>
<h3 id='other-artifacts'><strong>Other Artifacts</strong></h3>
<p>All other files/folders.</p>
<h2 id='step-3-test-the-repo-by-dryrun'>Step 3: Test the Repo by Dryrun</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">api_url</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
    <span class="n">artifacts_path</span> <span class="o">=</span> <span class="s">"./aibro_repo"</span><span class="p">,</span>
    <span class="n">dryrun</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>Dryrun locally validates the repo structure and tests if inference result can be successfully returned.</p>
<h2 id='step-4-create-an-inference-api-with-one-line-code'>Step 4: Create an inference API with one-line code</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">api_url</span> <span class="o">=</span> <span class="n">Inference</span><span class="p">.</span><span class="n">deploy</span><span class="p">(</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s">"my_fancy_transformer"</span><span class="p">,</span>
    <span class="n">machine_id_config</span> <span class="o">=</span> <span class="s">"c5.large.od"</span><span class="p">,</span>
    <span class="n">artifacts_path</span> <span class="o">=</span> <span class="s">"./aibro_repo"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>Assume the formatted model repo is saved at path &quot;./aibro_repo&quot;, we can now use it to create an inference job. The model name should be unique with respect to all current <a href="https://aipaca.ai/inference_jobs">active inference jobs</a> under your profile.</p>

<p>In this example, we deployed a public custom model from &quot;./aibro_repo&quot; called &quot;my_fancy_transformer&quot; on machine type &quot;c5.large.od&quot; and used an access token for authentication.</p>

<p>Once the deployment is finished, an API URL is returned with the syntax: </br></p>

<ul>
<li><strong>https://api.aipaca.ai/v1/{username}/{client_id}/{model_name}/predict</strong> </br></li>
</ul>

<p><strong>{client_id}</strong>: if your inference job is public, <strong>{client_id}</strong> is filled by &quot;public&quot;. Otherwise, <strong>{client_id}</strong> should be filled by one of your <a href="#add-clients">clients&#39; ID</a>.</p>
<h2 id='step-5-test-a-aibro-api-with-curl'>Step 5: Test a Aibro API with curl:</h2>
<p>The syntax when using <code>curl</code> depends on the file type in the <code>data</code> folder.</p>

<table><thead>
<tr>
<th>Data Type</th>
<th>syntax</th>
</tr>
</thead><tbody>
<tr>
<td>json</td>
<td>curl -X POST {{aibro url}} -d &#39;{&quot;your&quot;: &quot;data&quot;}&#39;<br/>curl -X POST {{aibro url}} -F file=@&#39;path/to/json/file&#39;</td>
</tr>
<tr>
<td>txt</td>
<td>curl -X POST {{aibro url}} -d &#39;your data&#39;<br/>curl -X POST {{aibro url}} -F file=@&#39;path/to/txt/file&#39;</td>
</tr>
<tr>
<td>csv</td>
<td>curl -X POST {{aibro url}} -F file=@&#39;path/to/csv/file&#39;</td>
</tr>
<tr>
<td>others</td>
<td>curl -X POST {{aibro url}} -F file=@&#39;path/to/zip/file&#39;</td>
</tr>
</tbody></table>

<p>You may have observed some patterns from the syntax lookup table above:</p>

<ul>
<li>If the data type is <code>json</code> or <code>txt</code>, you could use <code>-d</code> flag to post the string data directly.</li>
<li>If the data type is one of <code>json</code>, <code>txt</code>, or <code>csv</code>, you could use <code>-F</code> flag to post the data file by path.</li>
<li>If the data type is not one of <code>json</code>, <code>txt</code>, or <code>csv</code>, you could zip the entire <code>data</code> folder then post the data file by the zip path.</li>
</ul>

<p><strong>The posted data will replace everything in the <code>data</code> folder. Therefore, your posted data should have the same format as whatever you had in the <code>data</code> folder initially.</strong></p>

<p><em>Tips</em>: if your inference time is over one minute, it is recommended to either reduce the data size or increase the <code>--keepalive-time</code> value when using <code>curl</code>.</p>
<h2 id='step-6-limit-api-access-to-specific-clients-optional'>Step 6: Limit API Access to Specific Clients (Optional)</h2>
<p>As the API owner, you probably don&#39;t receive overwhelming API requests from everywhere. To avoid this trouble, you could give every client a unique client id, which is going to be used in API endpoints (as the showing syntax in step 4). If no client id is added, this inference job would become public.</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">Inference</span><span class="p">.</span><span class="n">update_clients</span><span class="p">(</span>
    <span class="n">job_id</span><span class="p">,</span>
    <span class="n">add_client_ids</span> <span class="o">=</span> <span class="p">[</span><span class="s">"client_1"</span><span class="p">,</span> <span class="s">"client_2"</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div><h2 id='step-7-complete-job'>Step 7: Complete Job</h2>
<p>Once the inference job is no longer used, to avoid unnecessary cost, please remember to close it by <code>Inference.complete()</code>.</p>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">from</span> <span class="nn">aibro</span> <span class="kn">import</span> <span class="n">Inference</span>
<span class="n">Inference</span><span class="p">.</span><span class="n">complete</span><span class="p">(</span><span class="n">job_id</span><span class="p">)</span>
</code></pre></div><h1 id='aibro-inference'>aibro.Inference</h1><h2 id='deploy'>deploy()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">deploy</span><span class="p">(</span>
    <span class="n">artifacts_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">machine_id_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">dryrun</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">cool_down_period_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">600</span><span class="p">,</span>
    <span class="n">client_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">""</span><span class="p">,</span>
    <span class="n">wait_request_s</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</code></pre></div>
<p>This method starts an inference job to deploy models on cloud instances.</p>
<h3 id='parameters'>Parameters</h3>
<p><strong>artifacts_path</strong>: <em>Union[str, list]</em> <br/>
The path to the formatted repo. The input type can be either string or dictionary:</p>

<table><thead>
<tr>
<th>Type</th>
<th>Syntax</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>str</td>
<td><code>&quot;path/to/repo&quot;</code></td>
<td>The whole repo would be uploaded and deployed on cloud</td>
</tr>
<tr>
<td>list</td>
<td><code>[&quot;./path/to/model&quot;,</code><br/> <code>&quot;./path/to/predict.py&quot;,</code><br/> <code>&quot;./path/to/data&quot;,</code><br/> <code>&quot;./path/to/requirements.txt&quot;,</code><br/> <code>&quot;./path/to/other_artifacts&quot;]</code></td>
<td>Select important artifacts one-by-one. AIbro would combine and recreate a model repo called <code>aibro_repo</code> under the root path of aibro library</td>
</tr>
</tbody></table>

<p><strong>model_name</strong>: <em>str</em> = None <br/>
The model name used in the deployed inference job. It could only be None iff <code>dryun==False</code>. In the scope of user profile, the model name should be unique with respect to the model names from all active inference jobs.</p>

<p><strong>machine_id_config</strong>: <em>Union[str, dict]</em> <br/>
The machine configuration used to deploy the model. It could only be None iff <code>dryun==False</code>. The input type can be either string or dictionary:</p>

<table><thead>
<tr>
<th>Type</th>
<th>Syntax</th>
<th>Meaning</th>
</tr>
</thead><tbody>
<tr>
<td>str</td>
<td><code>&quot;c5.large.od&quot;</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance</td>
</tr>
<tr>
<td>dict</td>
<td><code>{&quot;standby&quot;: &quot;c5.large.od&quot;}</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance</td>
</tr>
<tr>
<td>dict</td>
<td><code>{&quot;standby&quot;: &quot;c5.large.od&quot;, &quot;cooling&quot;: &quot;g4dn.4xlarge.od&quot;}</code></td>
<td>use the on-demand &quot;c5.large&quot; instance as the standby instance and the on-demand &quot;g4dn.4xlarge&quot; instance as the cooling instance</td>
</tr>
</tbody></table>

<p><u>Important:</u></p>

<ol>
<li><u>The machine id has to be on-demand (ends by <code>.od</code>).</u></li>
<li><u>One standby instance is mandatory whereas cooling instance is optional (e.g. syntax such as {&quot;cooling&quot;: &quot;g4dn.4xlarge.od&quot;} is invalid).</u></li>
</ol>

<p>In the configuration, machines are categorized into &quot;Standby&quot; and &quot;Cooling&quot;. In general, their definitions are below:</p>

<ul>
<li>Standby instance: the instance never turns off.</li>
<li>Cooling instance: the instance only turns on when receiving a new inference request and only turns off when no inference request is received within <code>cool_down_period_s</code> seconds.</li>
</ul>

<p>For more details about the usage of standby and cooling instances, check out the <a href="#standby-and-cooling-instances">section below</a>.</p>

<aside class="notice">
<span style="font-weight: bold">FAQ: Why machine_id_config only accept on-demand instances? Isn't it more expensive than the spot?</span> <br/>

Answer: spot instance is not always available, using on-demand instance is to ensure that the inference job can always get a server. If spot instances are available, AIbro automatically replaces its same-type on-demand instances to save your cost.

</aside>

<p><strong>dryrun</strong>: <em>bool = False</em><br/>
The argument used to test model repo locally. If <code>dryrun==True</code>, <code>deploy()</code> would start validating the repo structure and testing if the inference result can be successfully returned.</p>

<p><strong>cool_down_period_s</strong>: <em>int = 600</em><br/>
The cool down period of cooling instance. By default, the cooling instance would stop once there is no new inference request coming within 600 seconds.</p>

<p><strong>client_ids</strong>: <em>List[str] = []</em><br/>
The argument used to restrict specific clients to access to your inference API. The client ids can be customized in any syntax as long as there is no duplication. If there is no client id, the inference job becomes public. <code>Inference.update_clients()</code> can be used to add/remove client ids. <br/>
As mentioned in the <a href="#step-3-create-an-inference-api-with-one-line-code">tutorial</a>, client id is used as a part of the API URL. <br/>
The table below clarifies the definitions of roles.</p>

<table><thead>
<tr>
<th>Role</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>API owner</td>
<td>People who created the inference API</td>
</tr>
<tr>
<td>API client</td>
<td>People who have the access to an inference API</td>
</tr>
</tbody></table>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token used in <a href="#authentication">authentication</a>. If its value is None, email and password would be required to input.</p>

<p><strong>description</strong>: <em>str</em> = &quot;&quot; <br/>
The description used to remind which inference job was which.</p>

<p><strong>wait_request_s</strong>: <em>int = 30</em><br/>
The time in seconds used to wait for instance requests to be fulfilled.</p>
<h2 id='complete'>complete()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">complete</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">):</span>
</code></pre></div>
<p>Completing an inference job shuts down all instances and stops its API services.</p>
<h3 id='parameters-2'>Parameters</h3>
<p><strong>model_name</strong>: <em>str</em> = None <br/>
The model name used in the deployed inference job. Since a model name is required to be unique, it can be used to search which job to complete. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
The job_id of the deployed inference job. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token used in <a href="#authentication">authentication</a>. If its value is None, email and password would be required to input.</p>
<h2 id='update_clients'>update_clients()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">update_clients</span><span class="p">(</span>
        <span class="n">add_client_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">remove_client_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">be_public</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span><span class="o">-&gt;</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</code></pre></div>
<p>Update the client ids of the targeted inference job.</p>
<h3 id='parameters-3'>Parameters</h3>
<p><strong>add_client_ids</strong>: <em>Union[str, List[str]]</em> = [] <br/>
The argument used to add one single or one list of client ids. If duplicated client ids are found, the update would be canceled.</p>

<p><strong>remove_client_ids</strong>: <em>Union[str, List[str]]</em> = [] <br/>
The argument used to remove one single or one list of client ids. Client ids that are not found would be ignored.</p>

<p><strong>be_public</strong>: <em>bool</em> = False <br/>
If the argument is set <code>True</code>, it removes all client ids and the inference job becomes public.</p>

<p><strong>model_name</strong>: <em>str</em> = None <br/>
The model name used in the deployed inference job. Since a model name is required to be unique, it can be used to search the targeted inference job. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
The job_id of the deployed inference job. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token used in <a href="#authentication">authentication</a>. If its value is None, email and password would be required to input.</p>
<h2 id='list_clients'>list_clients()</h2><div class="highlight"><pre class="highlight python tab-python"><code><span class="k">def</span> <span class="nf">list_clients</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">job_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">access_token</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span><span class="o">-&gt;</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
</code></pre></div>
<p>Return a list of client ids of the targeted inference job.</p>
<h3 id='parameters-4'>Parameters</h3>
<p><strong>model_name</strong>: <em>str</em> = None <br/>
The model name used in the deployed inference job. Since a model name is required to be unique, it can be used to search the targeted inference job. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>job_id</strong>: <em>str</em> = None <br/>
The job_id of the deployed inference job. You may only need to input either <code>model_name</code> or <code>job_id</code>. If both parameters have input, <code>job_id</code> would be used to search inference jobs.</p>

<p><strong>access_token</strong>: <em>str</em> = None <br/>
The access token used in <a href="#authentication">authentication</a>. If its value is None, email and password would be required to input.</p>
<h2 id='standby-and-cooling-instances'>Standby and Cooling instances</h2><h3 id='definitions'>Definitions:</h3>
<ul>
<li>Standby instance: the instance never turns off.</li>
<li>Cooling instance: the instance only turns on when receiving a new inference request and only turns off when no inference request is received within <code>cool_down_period_s</code> seconds.</li>
</ul>

<p>If both standby and cooling instances are turned on, the cooling instance would have a higher priority to handle upcoming inference requests.</p>
<h3 id='configure-for-non-uniform-traffic'>Configure for non-uniform traffic</h3>
<p>In simple words, the combination of standby and cooling instance configuration is a cross-instance scaling strategy. Let&#39;s say you want to build an inference API for a web application. The call frequency is proportional to the site traffic, which is usually non-uniform (e.g. more traffic during the day and a few during the night). To take the advantage of the configuration, you may set a CPU or cheaper GPU instance as the standby instance and set a powerful GPU instance as the cooling instance. In this way, the powerful cooling instance would efficiently handle intensive traffic during the day and the standby instance would save your money during the night and still keep the near real-time performance.</p>
<h3 id='configure-for-high-traffic'>Configure for high traffic</h3>
<p>On the other hand, if your site traffic is more uniform or people just never stop using it, it is more recommended to only use standby instances with the powerful GPU. Why is that? if you kept the previous configuration which uses a weak standby and a powerful cooling, the cooling instance may just never end its cooling period since there are always requests coming. In this case, the weak standby instance is not applied at all and is wasted.</p>
<h3 id='case-study-calculate-the-saving'>Case study: calculate the saving</h3>
<p><img src="https://drive.google.com/uc?export=view&amp;id=1sLxQ6SZDamjT9qGp2JE7zElUZwIpLQ3I" alt="" /></p>

<p><strong>Graph explanation</strong>: Given that you created an inference job with configuration <code>machine_id_config = {&quot;standby&quot;: &quot;g4dn.4xlarge&quot;, &quot;cooling&quot;: &quot;g4dn.12xlarge.od&quot;}</code>. The inference job was applied from 8:00 am to 11.59 pm. Most inference requests (IRs) were received during the morning and afternoon. Every time an IR was received by the cooling instance, the instance&#39;s cooldown period reset. If the cooldown period passed by without receiving one IR, the cooling instance stopped, and the standby instance handled the next IR. At that time, the standby started invoking back the stopped cooling instance. The standby instance also processed the following IRs until the cooling instance fully woke up. Within the 16 hours, cooling instances were turned on for 6 hours and the standby instance was never turned off. The machine pricing is shown below:</p>

<table><thead>
<tr>
<th>Machine id</th>
<th>Pricing</th>
</tr>
</thead><tbody>
<tr>
<td>g4dn.12xlarge.od</td>
<td>$3.91/hr</td>
</tr>
<tr>
<td>g4dn.4xlarge.od</td>
<td>$1.20/hr</td>
</tr>
<tr>
<td>g4dn.12xlarge</td>
<td>$1.61/hr</td>
</tr>
<tr>
<td>g4dn.4xlarge</td>
<td>$0.36/hr</td>
</tr>
</tbody></table>

<p>Let&#39;s compare the saving with and without standby&amp;cooling configuration.</p>

<table><thead>
<tr>
<th>Configuration</th>
<th>machine_id_config</th>
<th>Cost</th>
<th>Spot Cost</th>
</tr>
</thead><tbody>
<tr>
<td>Without standby&amp;cooling</td>
<td><code>{&quot;standby&quot;: &quot;g4dn.12xlarge.od&quot;}</code></td>
<td>3.91 * 16 = $62.56</td>
<td>$25.76</td>
</tr>
<tr>
<td>With standby&amp;cooling</td>
<td><code>{&quot;standby&quot;: &quot;g4dn.4xlarge&quot;,</code><br/><code>&quot;cooling&quot;: &quot;g4dn.12xlarge.od&quot;}</code></td>
<td>1.2_ 16 + 3.91 * 6 = $42.66</td>
<td>$15.42</td>
</tr>
</tbody></table>

<p>In this scenario, the pure saving from standby&amp;cooling configuration was over 30%. Plus, if their spot instances were available over the period, AIbro would end up cutting the cost from $62.56 to $15.42. That was a 75% saving!</p>
<h2 id='inference-job-amp-instance-status'>Inference Job &amp; Instance Status</h2>
<p>Once a job starts, its states and substates are updated on the <a href="https://aipaca.ai/inference_jobs">Jobs page of AIbro Console</a>.</p>

<table><thead>
<tr>
<th>Job Status</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>QUEUING</td>
<td>Waiting to be deployed</td>
</tr>
<tr>
<td>DEPLOYED</td>
<td>Inference API is ready to be used</td>
</tr>
<tr>
<td>CANCELED</td>
<td>Canceled due to some errors</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>Job was completed</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Job Substatus</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>REQUESTING SERVER</td>
<td>Requesting an instance to deploy models</td>
</tr>
<tr>
<td>CONNECTING SERVER</td>
<td>Connecting an initializing instance</td>
</tr>
<tr>
<td>GEARING UP ENV</td>
<td>Gearing up tensorflow and mounting GPUs</td>
</tr>
<tr>
<td>DEPLOYING MODEL</td>
<td>Deploying model</td>
</tr>
<tr>
<td>DEPLOYED MODEL</td>
<td>Model was deployed</td>
</tr>
<tr>
<td>CANCELED</td>
<td>Canceled due to some errors</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>Completed the job</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Instance Status</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>LAUNCHING</td>
<td>Setting up instance for inference</td>
</tr>
<tr>
<td>EXECUTING</td>
<td>Having jobs in ready to receive inference requests</td>
</tr>
<tr>
<td>COOLING</td>
<td>Within cooling Period (must be a <a href="#standby-and-cooling-instances">cooling instance</a>)</td>
</tr>
<tr>
<td>CLOSING</td>
<td>Stopping/terminating instance</td>
</tr>
<tr>
<td>CLOSED</td>
<td>instance has been stopped/terminated</td>
</tr>
</tbody></table>

<table><thead>
<tr>
<th>Instance Substatus</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>STOPPING/STOPPED</td>
<td>Shut down instance but retain root volume <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">Reference</a></td>
</tr>
<tr>
<td>TERMINATING/TERMINATED</td>
<td>Completely delete the instance <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html">Reference</a></td>
</tr>
</tbody></table>

<p>The following table is a status-substatus map of jobs and instances.</p>

<table><thead>
<tr>
<th>Job Status</th>
<th>Job Substatus</th>
<th>Instance Status</th>
<th>Instance Substatus</th>
</tr>
</thead><tbody>
<tr>
<td>QUEUING</td>
<td>REQUESTING SERVER</td>
<td></td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>CONNECTING SERVER</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>GEARING UP ENV</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>QUEUING</td>
<td>DEPLOYING MODEL</td>
<td>LAUNCHING</td>
<td></td>
</tr>
<tr>
<td>----------</td>
<td>------------</td>
<td>----------------------</td>
<td>--------------------------</td>
</tr>
<tr>
<td>DEPLOYED</td>
<td>DEPLOYED</td>
<td>EXECUTING/COOLING</td>
<td></td>
</tr>
<tr>
<td>----------</td>
<td>------------</td>
<td>----------------------</td>
<td>--------------------------</td>
</tr>
<tr>
<td>CANCELED</td>
<td>CANCELED</td>
<td>COOLING/CLOSING/CLOSED</td>
<td>COOLING/(STOPPING, TERMINATING)/(STOPPED, TERMINATED)</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>COMPLETED</td>
<td>COOLING/CLOSING/CLOSED</td>
<td>COOLING/(STOPPING, TERMINATING)/(STOPPED, TERMINATED)</td>
</tr>
</tbody></table>
<h1 id='cloud-instance'>Cloud Instance</h1>
<p>We will use the word &quot;machine&quot; and &quot;server&quot; interchangeably with &quot;instance&quot; in the following content.</p>
<h2 id='spot-vs-on-demand-instance'>Spot Vs On-demand Instance</h2><h3 id='machine-id'>Machine id</h3>
<p>By simply adding &quot;.od&quot; after machine id to convert instance type from spot to on-demand (e.g. <em>p2.xlarge</em> is spot and <em>p2.xlarge.od</em> is on-demand).</p>
<h3 id='pricing'>Pricing</h3>
<p>Spot instances are usually 70% cheaper than their corresponding on-demand instances.</p>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1G6fVuxUu8Yofj_SBxl57m_lFZ83L4MzE" alt="" />
<img src="https://drive.google.com/uc?export=view&amp;id=1vomqhv0C7-ZjmwZ62dv3bjFJZtL2-wiT" alt="" /></p>
<h3 id='availability'>Availability</h3>
<p>As a tradeoff, spot instance requests are not always fulfilled. We defined the term <strong>&quot;availability&quot;</strong> as the success probability of
instance request.</p>

<p>Clearly, the availabilities of on-demand instances are always 100%. Therefore, it is guaranteed to get an on-demand instance as long as there is enough capacity in the AIbro marketplace. With a small possibility, AWS can run out of capacity itself, but it is not detectable until the request error occurs in jobs.</p>

<p>The availabilities of spot instances is varied by instance types and request time. In general, we found more powerful
instance types have less availability (e.g. p3.2xlarge is less available than p2.xlarge). Meanwhile, spot instances are
usually more available during non-working hours.</p>
<h3 id='reliability'>Reliability</h3>
<p>Spot instances have a chance to be interrupted by AWS. On-demand instances are always stable.</p>
<h1 id='contact-us'>Contact Us</h1>
<p>You could reach out to us in one of the following ways:</p>

<ol>
<li><strong>Most recommend</strong>: <a href="https://discord.gg/kEdtjUYb">Discord Community</a> to direct message our support team</li>
<li>Use the &quot;Contact Us&quot; button from our <a href="https://aipaca.ai">website</a></li>
<li>Send message by <a href="#send_message"><code>aibro.Comm.send_message()</code></a></li>
<li>Email us at <a href = "mailto: hello@aipaca.ai">hello@aipaca.ai</a></li>
</ol>
<h1 id='data-privacy'>Data Privacy</h1>
<p>AIbro inference only stores the metadata from inference requests for service improvement purposes.</p>

      </div>
      <div class="dark-box">
          <div class="lang-selector">
                <a href="#" data-language-name="python">python</a>
          </div>
      </div>
    </div>
  </body>
</html>
